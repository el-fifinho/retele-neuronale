{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import MNIST \n",
    "\n",
    "def download_mnist(is_train: bool):\n",
    "    dataset = MNIST(\n",
    "        root='./data',\n",
    "        train=is_train,\n",
    "        transform=lambda x: np.array(x).flatten(),\n",
    "        download=False\n",
    "    )\n",
    "\n",
    "    mnist_data = []\n",
    "    mnist_labels = []\n",
    "    for image,label in dataset:\n",
    "        mnist_data.append(image)\n",
    "        mnist_labels.append(label)\n",
    "\n",
    "    return np.array(mnist_data), np.array(mnist_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download data sets:**\n",
    " \n",
    "*train_X* = imagini de antrenament\n",
    "\n",
    "*train_Y* = labels pentru imagini de antrenament\n",
    "\n",
    "*test_X* = imagini pentru testare\n",
    "\n",
    "*test_Y* = labels pentru imagini de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79674/3851143839.py:8: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  transform=lambda x: np.array(x).flatten(),\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y = download_mnist(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79674/3851143839.py:8: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  transform=lambda x: np.array(x).flatten(),\n"
     ]
    }
   ],
   "source": [
    "test_X, test_Y = download_mnist(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Normalize the data and convert the labels to *one-hot-encoding*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_norm = train_X / 255\n",
    "test_X_norm = test_X / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder \n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore',sparse_output=False)\n",
    "train_Y_enc = enc.fit_transform(train_Y.reshape(-1,1))\n",
    "test_Y_enc = enc.fit_transform(test_Y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_batches(trainset,labels,batches_dimension):\n",
    "    num_batches = int(np.ceil(len(trainset) / batches_dimension))\n",
    "    \n",
    "    batches = []\n",
    "    for i in range(num_batches):\n",
    "        start_index = i * batches_dimension\n",
    "        end_index = min((i + 1) * batches_dimension, len(trainset))\n",
    "        batch_data = trainset[start_index:end_index]\n",
    "        batch_labels = labels[start_index:end_index]\n",
    "        batches.append((batch_data, batch_labels))\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "    return expZ / np.sum(expZ, axis=1, keepdims=True)\n",
    "\n",
    "def forward_propagation(X,W,b):\n",
    "    Z = np.dot(X,W) + b\n",
    "    A = softmax(Z)\n",
    "    return A\n",
    "\n",
    "def compute_loss(A,Y):\n",
    "    m = Y.shape[0]\n",
    "    log_likelihood = -np.log(A[range(m), Y.argmax(axis=1)])\n",
    "    return np.sum(log_likelihood) / m\n",
    "\n",
    "def backward_propagation(X,Y,A):\n",
    "    m = X.shape[0]\n",
    "    dz = A - Y\n",
    "    dW = np.dot(X.T, dz) / m\n",
    "    db = np.sum(dz, axis=0, keepdims=True) / m\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch,train_data, w, b):\n",
    "    X_batch, Y_batch = train_data\n",
    "    A = forward_propagation(X_batch, w, b)\n",
    "    new_w, new_b = backward_propagation(X_batch,Y_batch,A)\n",
    "\n",
    "    return new_w, new_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(W,b,dW,db,learning_rate):\n",
    "    W -= learning_rate * dW\n",
    "    b -= learning_rate * db\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weight = np.random.randn(784,10) * 0.01\n",
    "bias = np.zeros((1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done.\n",
      "Epoch 1 done.\n",
      "Epoch 2 done.\n",
      "Epoch 3 done.\n",
      "Epoch 4 done.\n",
      "Epoch 5 done.\n",
      "Epoch 6 done.\n",
      "Epoch 7 done.\n",
      "Epoch 8 done.\n",
      "Epoch 9 done.\n",
      "Epoch 10 done.\n",
      "Epoch 11 done.\n",
      "Epoch 12 done.\n",
      "Epoch 13 done.\n",
      "Epoch 14 done.\n",
      "Epoch 15 done.\n",
      "Epoch 16 done.\n",
      "Epoch 17 done.\n",
      "Epoch 18 done.\n",
      "Epoch 19 done.\n",
      "Epoch 20 done.\n",
      "Epoch 21 done.\n",
      "Epoch 22 done.\n",
      "Epoch 23 done.\n",
      "Epoch 24 done.\n",
      "Epoch 25 done.\n",
      "Epoch 26 done.\n",
      "Epoch 27 done.\n",
      "Epoch 28 done.\n",
      "Epoch 29 done.\n",
      "Epoch 30 done.\n",
      "Epoch 31 done.\n",
      "Epoch 32 done.\n",
      "Epoch 33 done.\n",
      "Epoch 34 done.\n",
      "Epoch 35 done.\n",
      "Epoch 36 done.\n",
      "Epoch 37 done.\n",
      "Epoch 38 done.\n",
      "Epoch 39 done.\n",
      "Epoch 40 done.\n",
      "Epoch 41 done.\n",
      "Epoch 42 done.\n",
      "Epoch 43 done.\n",
      "Epoch 44 done.\n",
      "Epoch 45 done.\n",
      "Epoch 46 done.\n",
      "Epoch 47 done.\n",
      "Epoch 48 done.\n",
      "Epoch 49 done.\n",
      "Epoch 50 done.\n",
      "Epoch 51 done.\n",
      "Epoch 52 done.\n",
      "Epoch 53 done.\n",
      "Epoch 54 done.\n",
      "Epoch 55 done.\n",
      "Epoch 56 done.\n",
      "Epoch 57 done.\n",
      "Epoch 58 done.\n",
      "Epoch 59 done.\n",
      "Epoch 60 done.\n",
      "Epoch 61 done.\n",
      "Epoch 62 done.\n",
      "Epoch 63 done.\n",
      "Epoch 64 done.\n",
      "Epoch 65 done.\n",
      "Epoch 66 done.\n",
      "Epoch 67 done.\n",
      "Epoch 68 done.\n",
      "Epoch 69 done.\n",
      "Epoch 70 done.\n",
      "Epoch 71 done.\n",
      "Epoch 72 done.\n",
      "Epoch 73 done.\n",
      "Epoch 74 done.\n",
      "Epoch 75 done.\n",
      "Epoch 76 done.\n",
      "Epoch 77 done.\n",
      "Epoch 78 done.\n",
      "Epoch 79 done.\n",
      "Epoch 80 done.\n",
      "Epoch 81 done.\n",
      "Epoch 82 done.\n",
      "Epoch 83 done.\n",
      "Epoch 84 done.\n",
      "Epoch 85 done.\n",
      "Epoch 86 done.\n",
      "Epoch 87 done.\n",
      "Epoch 88 done.\n",
      "Epoch 89 done.\n",
      "Epoch 90 done.\n",
      "Epoch 91 done.\n",
      "Epoch 92 done.\n",
      "Epoch 93 done.\n",
      "Epoch 94 done.\n",
      "Epoch 95 done.\n",
      "Epoch 96 done.\n",
      "Epoch 97 done.\n",
      "Epoch 98 done.\n",
      "Epoch 99 done.\n",
      "Epoch 100 done.\n",
      "Epoch 101 done.\n",
      "Epoch 102 done.\n",
      "Epoch 103 done.\n",
      "Epoch 104 done.\n",
      "Epoch 105 done.\n",
      "Epoch 106 done.\n",
      "Epoch 107 done.\n",
      "Epoch 108 done.\n",
      "Epoch 109 done.\n",
      "Epoch 110 done.\n",
      "Epoch 111 done.\n",
      "Epoch 112 done.\n",
      "Epoch 113 done.\n",
      "Epoch 114 done.\n",
      "Epoch 115 done.\n",
      "Epoch 116 done.\n",
      "Epoch 117 done.\n",
      "Epoch 118 done.\n",
      "Epoch 119 done.\n",
      "Epoch 120 done.\n",
      "Epoch 121 done.\n",
      "Epoch 122 done.\n",
      "Epoch 123 done.\n",
      "Epoch 124 done.\n",
      "Epoch 125 done.\n",
      "Epoch 126 done.\n",
      "Epoch 127 done.\n",
      "Epoch 128 done.\n",
      "Epoch 129 done.\n",
      "Epoch 130 done.\n",
      "Epoch 131 done.\n",
      "Epoch 132 done.\n",
      "Epoch 133 done.\n",
      "Epoch 134 done.\n",
      "Epoch 135 done.\n",
      "Epoch 136 done.\n",
      "Epoch 137 done.\n",
      "Epoch 138 done.\n",
      "Epoch 139 done.\n",
      "Epoch 140 done.\n",
      "Epoch 141 done.\n",
      "Epoch 142 done.\n",
      "Epoch 143 done.\n",
      "Epoch 144 done.\n",
      "Epoch 145 done.\n",
      "Epoch 146 done.\n",
      "Epoch 147 done.\n",
      "Epoch 148 done.\n",
      "Epoch 149 done.\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "from concurrent.futures import as_completed\n",
    "\n",
    "for epoch in range(150):\n",
    "    schuffle = np.random.permutation(train_X_norm.shape[0])\n",
    "    train_X_norm = train_X_norm[schuffle]\n",
    "    train_Y_enc = train_Y_enc[schuffle]\n",
    "\n",
    "    batches = split_into_batches(train_X_norm,train_Y_enc,100)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = {executor.submit(train, epoch, batch, Weight, bias): batch for batch in batches}\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            Weight, bias = update_params(Weight,bias,result[0],result[1],0.01)\n",
    "            \n",
    "    print(f'Epoch {epoch} done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,W,b):\n",
    "    A = forward_propagation(X,W,b)\n",
    "    return np.argmax(A, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 92.82%\n",
      "Testing Accuracy: 92.40%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_predictions = predict(train_X_norm,Weight,bias)\n",
    "test_predictions = predict(test_X_norm,Weight,bias)\n",
    "\n",
    "print(f'Training Accuracy: {accuracy_score(np.argmax(train_Y_enc,axis=1), train_predictions) * 100:.2f}%')\n",
    "print(f'Testing Accuracy: {accuracy_score(test_Y, test_predictions) * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
