{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import math\n",
    "from scipy.stats import bernoulli\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dataset(is_train: bool):\n",
    "    dataset = MNIST(\n",
    "        root='./data',\n",
    "        train=is_train,\n",
    "        transform=lambda x: np.array(x).flatten(),\n",
    "        download=False\n",
    "    )\n",
    "\n",
    "    mnist_data = []\n",
    "    mnist_labels = []\n",
    "    for image,label in dataset:\n",
    "        mnist_data.append(image)\n",
    "        mnist_labels.append(label)\n",
    "\n",
    "    return np.array(mnist_data), np.array(mnist_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels):\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    return encoder.fit_transform(labels.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_initialization_Xavier_Uniform(input_dim, output_dim):\n",
    "    low_bound = -math.sqrt(6/(input_dim + output_dim))\n",
    "    upper_bound = math.sqrt(6/(input_dim + output_dim))\n",
    "    W = np.random.uniform(low_bound, upper_bound, size=(input_dim, output_dim))\n",
    "    b = np.zeros((1, output_dim))\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "    return expZ / np.sum(expZ, axis=1, keepdims=True)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1/(1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    \n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_cross_entropy(A, Y):\n",
    "    m = Y.shape[0]\n",
    "    log_likelihood = -np.log(A[range(m), Y.argmax(axis=1)])\n",
    "    return np.sum(log_likelihood) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_no_dropout(X, W1, b1, W2, b2):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return A1, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W1, b1, W2, b2):\n",
    "    A2 = forward_propagation_no_dropout(X, W1, b1, W2, b2)[1]\n",
    "    return np.argmax(A2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = init_dataset(True)\n",
    "test_X, test_Y = init_dataset(False)\n",
    "    \n",
    "train_Y = one_hot_encode(train_Y)\n",
    "test_Y_one_hot = one_hot_encode(test_Y)\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "dropout_rate = 0.20\n",
    "    \n",
    "W1, b1 = weight_initialization_Xavier_Uniform(784, 100)\n",
    "W2, b2 = weight_initialization_Xavier_Uniform(100, 10)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    perm = np.random.permutation(train_X.shape[0])\n",
    "    train_X = train_X[perm]\n",
    "    train_Y = train_Y[perm]\n",
    "    \n",
    "    for i in range(0, train_X.shape[0], batch_size):\n",
    "        X_batch = train_X[i:i+batch_size]\n",
    "        Y_batch = train_Y[i:i+batch_size]\n",
    "        \n",
    "        # Forward propagation\n",
    "        Z1 = np.dot(X_batch, W1) + b1\n",
    "        Y1 = sigmoid(Z1)\n",
    "        dropouts = bernoulli.rvs(dropout_rate, size=Y1.shape) \n",
    "        Y1_dropout = Y1 * dropouts / (1 - dropout_rate)\n",
    "        Z2 = np.dot(Y1_dropout, W2) + b2\n",
    "        Y2 = softmax(Z2)\n",
    "        \n",
    "        # Compute loss (optional for tracking)\n",
    "        loss = compute_loss_cross_entropy(Y2, Y_batch)\n",
    "        \n",
    "        # Backward propagation\n",
    "        m = X_batch.shape[0]\n",
    "\n",
    "        error_Z2 = Y2 - Y_batch\n",
    "        dW2 = np.dot(Y1.T, error_Z2) / m  \n",
    "        db2 = np.sum(error_Z2, axis=0, keepdims=True) / m\n",
    "\n",
    "        error_Z1 = np.dot(error_Z2, W2.T) * Y1 * (1 - Y1)\n",
    "        \n",
    "        error_Z1 = error_Z1 * dropouts\n",
    "        error_Z1 /= (1 - dropout_rate)\n",
    "    \n",
    "        dW1 = np.dot(X_batch.T, error_Z1) / m \n",
    "        db1 = np.sum(error_Z1, axis=0, keepdims=True) / m  \n",
    "        \n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "        \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}')\n",
    "\n",
    "\n",
    "train_predictions = predict(train_X, W1, b1, W2, b2)\n",
    "test_predictions = predict(test_X, W1, b1, W2, b2)\n",
    "    \n",
    "print(f'Training Accuracy: {accuracy_score(np.argmax(train_Y, axis=1), train_predictions) * 100:.2f}%')\n",
    "print(f'Test Accuracy: {accuracy_score(test_Y, test_predictions) * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
